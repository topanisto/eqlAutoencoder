{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lorenz EQL autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc4mYzqztFtb",
        "outputId": "5d46731a-d987-4a92-b675-72b5ba69562e"
      },
      "source": [
        "!pip install pyyaml h5py "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BapBiwz3Pn0s"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6cST34hVK-u"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.integrate import odeint\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow.nn as nn\n",
        "\n",
        "import torch \n",
        "import torch.nn \n",
        "\n",
        "from inspect import signature\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy.special import legendre, binom\n",
        "\n",
        "\n",
        "#generating the dataset: high dimensional data from low dimensions, legendre polynomials from Lorenz encoder\n",
        "\n",
        "def lorenz_dynamics(s0, t, sigma=10, beta=8/3, rho=28):\n",
        "  \"simulating the dynamics\"\n",
        "  \"s0: list, 3 initial conditions\"\n",
        "  \"t = list, time points\"\n",
        "  \"output: arrays of trajectory, with first, 2nd derivatives\"\n",
        "\n",
        "  f = lambda s, t: [ -1*sigma*(s[0]-s[1]), rho*s[0]-s[1]-s[0]*s[2],\n",
        "                    s[0]*s[1]-beta*s[2]]\n",
        "  df = lambda s, ds, t: [sigma*(ds[1]-ds[0]),\n",
        "                         rho*ds[0]-ds[1]-ds[0]*s[2]-s[0]*ds[2],\n",
        "                         ds[0]*s[1]+s[0]*ds[1]-beta*ds[2]]\n",
        "\n",
        "  s = odeint(f, s0, t)\n",
        "\n",
        "  dt = t[1]-t[0]\n",
        "\n",
        "  ds = np.array([f(s[i],dt) for i in range(t.size)])\n",
        "  dds = np.array([df(s[i], ds[i], dt) for i in range(t.size)])\n",
        "\n",
        "  return s, ds, dds\n",
        "\n",
        "\n",
        "def generate_lorenz(initials, t, n, linear=True, normalization=None, sigma=10,\n",
        "                    beta=8/3, rho=28):\n",
        "  \"generate the high dimensional dataset\"\n",
        "  \"initials = kx3 array of k initial conditions\"\n",
        "  \"t = timesteps\"\n",
        "  \"n = size of high dim dataset\"\n",
        "  \"linear = is dataset linear combination of lorenz? or include cubic modes\"\n",
        "  \"output: big dictionary, x has dimension n\"\n",
        "  \"x is high dim dataset, s is low\"\n",
        "\n",
        "  n_ics = initials.shape[0]\n",
        "  nsteps = t.size\n",
        "  dt = t[1]-t[0]\n",
        "\n",
        "  d=3\n",
        "\n",
        "  setup = np.array([lorenz_dynamics(a,t) for a in initials])\n",
        "  s = setup[:, 0] #shape (n_ics, nsteps, d)\n",
        "  ds = setup[:, 1]\n",
        "  dds = setup[:, 2]\n",
        "  \n",
        "\n",
        "  if normalization is not None:\n",
        "    s *= normalization\n",
        "    ds *= normalization\n",
        "    dds *= normalization\n",
        "\n",
        "  L = 1\n",
        "  y_spatial = np.linspace(-1*L, L, n)\n",
        "\n",
        "  modes = np.array([legendre(i)(y_spatial) for i in range(2*d)]) #shape(2d, n)\n",
        "\n",
        "#figuring stuff out\n",
        "\n",
        "  s_lin = np.repeat(s, n).reshape(n_ics, nsteps, d, n)\n",
        "  s_sq = np.array([a ** 2 for a in s_lin])\n",
        "  s_cub = np.array([a ** 3 for a in s_lin])\n",
        "\n",
        "  ds_lin = np.repeat(ds, n).reshape(n_ics, nsteps, d,n)\n",
        "  ds_sq = np.array([a ** 2 for a in ds_lin])\n",
        "  dds_lin = np.repeat(dds, n).reshape(n_ics, nsteps, d,n)\n",
        "\n",
        "  x1 = np.multiply(s_lin[:,:,0, :], modes[0])\n",
        "  x2 = np.multiply(s_lin[:,:,1, :], modes[1])\n",
        "  x3 = np.multiply(s_lin[:,:,2, :], modes[2])\n",
        "  x4 = np.multiply(s_cub[:,:,0, :], modes[3])\n",
        "  x5 = np.multiply(s_cub[:,:,1, :], modes[4])\n",
        "  x6 = np.multiply(s_cub[:,:,2, :], modes[5])\n",
        "\n",
        "  dx1 = np.multiply(ds_lin[:,:,0, :], modes[0])\n",
        "  dx2 = np.multiply(ds_lin[:,:,1, :], modes[1])\n",
        "  dx3 = np.multiply(ds_lin[:,:,2, :], modes[2])\n",
        "  dx4 = np.multiply(np.multiply(ds_lin[:,:,0, :], modes[3]), s_sq[:,:,0,:])\n",
        "  dx5 = np.multiply(np.multiply(ds_lin[:,:,1, :], modes[4]), s_sq[:,:,2,:])\n",
        "  dx6 = np.multiply(np.multiply(ds_lin[:,:,2, :], modes[5]), s_sq[:,:,1,:])\n",
        "\n",
        "  ddx1 = np.multiply(dds_lin[:,:,0, :], modes[0])\n",
        "  ddx2 = np.multiply(dds_lin[:,:,1, :], modes[1])\n",
        "  ddx3 = np.multiply(dds_lin[:,:,2, :], modes[2])\n",
        "  ddx4 = np.multiply(dds_lin[:,:,0, :], modes[3])\n",
        "  ddx5 = np.multiply(dds_lin[:,:,1, :], modes[4])\n",
        "  ddx6 = np.multiply(dds_lin[:,:,2, :], modes[5])\n",
        "\n",
        "\n",
        "  x = x1 + x2 + x3\n",
        "  dx = dx1 + dx2 + dx3\n",
        "  ddx = ddx1 + ddx2 + ddx3\n",
        "\n",
        "  if not linear:\n",
        "    x += x4 + x5 + x6\n",
        "    dx +=  3* (dx4+ + dx5 + dx6)   \n",
        "    ddx += np.multiply(ddx4,s_sq[:,:,0,:]) \\\n",
        "    +2*np.multiply(ds_sq[:,:,0,:], s_lin[:,:,0,:]) \\\n",
        "        +np.multiply(ddx5,s_sq[:,:,1,:]) \\\n",
        "    +2*np.multiply(ds_sq[:,:,1,:], s_lin[:,:,1,:]) \\\n",
        "        +np.multiply(ddx6,s_sq[:,:,2,:]) \\\n",
        "    +2*np.multiply(ds_sq[:,:,2,:], s_lin[:,:,2,:]) \n",
        "  \n",
        "\n",
        "  data = {}\n",
        "  data[\"t\"] = t\n",
        "  data[\"y_spatial\"] = y_spatial\n",
        "  data[\"modes\"] = modes\n",
        "  data[\"x\"] = x\n",
        "  data[\"dx\"] = dx\n",
        "  data[\"ddx\"] = ddx\n",
        "  data[\"s\"] = s\n",
        "  data[\"ds\"] = ds\n",
        "  data[\"dds\"] = dds\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "def generate_dataset(n_ics, noise_strength=0):\n",
        "  t = np.arange(0, 10, 0.01) #returns array, no len, use size instead\n",
        "  steps = t.size\n",
        "  input_dim = 128\n",
        "\n",
        "    \n",
        "  ic_means = np.array([0,0,25])\n",
        "  ic_widths = 2*np.array([18,24,36])\n",
        "\n",
        "  # training data\n",
        "  ics = ic_widths*(np.random.rand(n_ics, 3)-.5) + ic_means\n",
        "  data = generate_lorenz(ics, t, input_dim, linear=False, normalization=np.array([1/40,1/40,1/40]))\n",
        "\n",
        "  data['x'] = data['x'].reshape((-1,input_dim)) + noise_strength*np.random.randn(steps*n_ics,input_dim)\n",
        "  data['dx'] = data['dx'].reshape((-1,input_dim)) + noise_strength*np.random.randn(steps*n_ics,input_dim)\n",
        "  data['ddx'] = data['ddx'].reshape((-1,input_dim)) + noise_strength*np.random.randn(steps*n_ics,input_dim)\n",
        "\n",
        "\n",
        "  return data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4VJKQ-OV4hF"
      },
      "source": [
        "#generating data \n",
        "\n",
        "noise_strength= 1e-6\n",
        "training_data = generate_dataset(128, noise_strength=noise_strength)\n",
        "validation_data = generate_dataset(20, noise_strength=noise_strength)\n",
        "\n",
        "params={}\n",
        "\n",
        "params['input_dim'] = 128\n",
        "params['latent_dim'] = 3\n",
        "params['model_order'] = 1\n",
        "params['poly_order'] = 3\n",
        "params['include_sine'] = False\n",
        "\n",
        "\n",
        "\n",
        "params['coefficient_threshold'] = 0.1\n",
        "params['threshold_frequency'] = 500\n",
        "\n",
        "params['coefficient_initialization'] = 'constant'\n",
        "\n",
        "params['loss_weight_decoder'] = 1\n",
        "params['loss_weight_eql_z'] = 1e-3\n",
        "params[\"loss_weight_eql_x\"] = 1e-3\n",
        "params[\"loss_weight_eql_regularization\"] = 1e-3\n",
        "params['activation'] = 'sigmoid'\n",
        "\n",
        "params['widths'] = [64, 32]\n",
        "params['epoch_size'] = training_data['x'].shape[0]\n",
        "params['batch_size'] = 32\n",
        "params['learning_rate'] = 1e-4\n",
        "params['data_path'] = os.getcwd() + '/'\n",
        "\n",
        "params['print_progress'] = True\n",
        "params['print_frequency'] = 100\n",
        "params['max_epochs'] = 1001\n",
        "params['refinement_epochs'] = 1001\n",
        "\n",
        "#eql things\n",
        "\n",
        "params[\"results_dir\"] = \"results/benchmark/test\"\n",
        "params[\"n_layers\"] = 2\n",
        "params[\"reg_weight\"] = 1e-2\n",
        "params[\"learning_rate_1\"] = 1e-2\n",
        "params[\"n_epochs1\"] = 20001\n",
        "params[\"n_epochs2\"] = 10001\n",
        "params[\"split\"] = 0.8"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xockFyH8MDSM"
      },
      "source": [
        "# EQL things"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05iTImA1MnWL"
      },
      "source": [
        "functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z_QxXhtEEPh"
      },
      "source": [
        "import sympy as sp\n",
        "\n",
        "class BaseFunction:\n",
        "    \"\"\"Abstract class for primitive functions\"\"\"\n",
        "    def __init__(self, norm=1):\n",
        "        self.norm = norm\n",
        "\n",
        "    def sp(self, x):\n",
        "        \"\"\"Sympy implementation\"\"\"\n",
        "        return None\n",
        "\n",
        "    def tf(self, x):\n",
        "        \"\"\"Automatically convert sympy to TensorFlow\"\"\"\n",
        "        z = sp.symbols('z')\n",
        "        return sp.utilities.lambdify(z, self.sp(z), 'tensorflow')(x)\n",
        "\n",
        "    def np(self, x):\n",
        "        \"\"\"Automatically convert sympy to numpy\"\"\"\n",
        "        z = sp.symbols('z')\n",
        "        return sp.utilities.lambdify(z, self.sp(z), 'numpy')(x)\n",
        "\n",
        "    def name(self, x):\n",
        "        return str(self.sp)\n",
        "\n",
        "\n",
        "class Constant(BaseFunction):\n",
        "    def tf(self, x):\n",
        "        return tf.ones_like(x)\n",
        "\n",
        "    def sp(self, x):\n",
        "        return 1\n",
        "\n",
        "    def np(self, x):\n",
        "        return np.ones_like\n",
        "\n",
        "\n",
        "class Identity(BaseFunction):\n",
        "    def tf(self, x):\n",
        "        return tf.identity(x) / self.norm\n",
        "\n",
        "    def sp(self, x):\n",
        "        return x / self.norm\n",
        "\n",
        "    def np(self, x):\n",
        "        return np.array(x) / self.norm\n",
        "\n",
        "\n",
        "class Square(BaseFunction):\n",
        "    def tf(self, x):\n",
        "        return tf.square(x) / self.norm\n",
        "\n",
        "    def sp(self, x):\n",
        "        return x ** 2 / self.norm\n",
        "\n",
        "    def np(self, x):\n",
        "        return np.square(x) / self.norm\n",
        "\n",
        "\n",
        "class Pow(BaseFunction):\n",
        "    def __init__(self, power, norm=1):\n",
        "        BaseFunction.__init__(self, norm=norm)\n",
        "        self.power = power\n",
        "\n",
        "    def sp(self, x):\n",
        "        return x ** self.power / self.norm\n",
        "\n",
        "    def tf(self, x):\n",
        "        return tf.pow(x, self.power) / self.norm\n",
        "\n",
        "\n",
        "class Sin(BaseFunction):\n",
        "    def sp(self, x):\n",
        "        return sp.sin(x * 2*2*np.pi) / self.norm\n",
        "\n",
        "\n",
        "class Sigmoid(BaseFunction):\n",
        "    # def tf(self, x):\n",
        "    #     return tf.sigmoid(x) / self.norm\n",
        "\n",
        "    def sp(self, x):\n",
        "        return 1 / (1 + sp.exp(-20*x)) / self.norm\n",
        "\n",
        "    def np(self, x):\n",
        "        return 1 / (1 + np.exp(-20*x)) / self.norm\n",
        "\n",
        "    def name(self, x):\n",
        "        return \"sigmoid(x)\"\n",
        "\n",
        "\n",
        "class Exp(BaseFunction):\n",
        "    def __init__(self, norm=np.e):\n",
        "        super().__init__(norm)\n",
        "\n",
        "    def sp(self, x):\n",
        "        return (sp.exp(x) - 1) / self.norm\n",
        "\n",
        "\n",
        "class Log(BaseFunction):\n",
        "    def sp(self, x):\n",
        "        return sp.log(sp.Abs(x)) / self.norm\n",
        "\n",
        "\n",
        "class BaseFunction2:\n",
        "    \"\"\"Abstract class for primitive functions with 2 inputs\"\"\"\n",
        "    def __init__(self, norm=1.):\n",
        "        self.norm = norm\n",
        "\n",
        "    def sp(self, x, y):\n",
        "        \"\"\"Sympy implementation\"\"\"\n",
        "        return None\n",
        "\n",
        "    def tf(self, x, y):\n",
        "        \"\"\"Automatically convert sympy to TensorFlow\"\"\"\n",
        "        a, b = sp.symbols('a b')\n",
        "        return sp.utilities.lambdify([a, b], self.sp(a, b), 'tensorflow')(x, y)\n",
        "\n",
        "    def np(self, x, y):\n",
        "        \"\"\"Automatically convert sympy to numpy\"\"\"\n",
        "        a, b = sp.symbols('a b')\n",
        "        return sp.utilities.lambdify([a, b], self.sp(a, b), 'numpy')(x, y)\n",
        "\n",
        "    def name(self, x, y):\n",
        "        return str(self.sp)\n",
        "\n",
        "\n",
        "class Product(BaseFunction2):\n",
        "    def __init__(self, norm=0.1):\n",
        "        super().__init__(norm=norm)\n",
        "\n",
        "    def sp(self, x, y):\n",
        "        return x*y / self.norm\n",
        "\n",
        "\n",
        "def count_inputs(funcs):\n",
        "    i = 0\n",
        "    for func in funcs:\n",
        "        if isinstance(func, BaseFunction):\n",
        "            i += 1\n",
        "        elif isinstance(func, BaseFunction2):\n",
        "            i += 2\n",
        "    return i\n",
        "\n",
        "\n",
        "def count_double(funcs):\n",
        "    i = 0\n",
        "    for func in funcs:\n",
        "        if isinstance(func, BaseFunction2):\n",
        "            i += 1\n",
        "    return i\n",
        "\n",
        "\n",
        "# default_func = [\n",
        "#     Constant(),\n",
        "#     Constant(),\n",
        "#     Identity(),\n",
        "#     Identity(),\n",
        "#     Square(),\n",
        "#     Square(),\n",
        "#     Sin(),\n",
        "#     Sigmoid(),\n",
        "# ]\n",
        "\n",
        "default_func = [\n",
        "    *[Constant()] * 2,\n",
        "    *[Identity()] * 4,\n",
        "    *[Square()] * 4,\n",
        "    *[Sin()] * 2,\n",
        "    *[Exp()] * 2,\n",
        "    *[Sigmoid()] * 2,\n",
        "    *[Product()] * 2,\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJJ7WmvnMzEa"
      },
      "source": [
        "def apply_activation(W, funcs, n_double=0):\n",
        "    \"\"\"Given an (n, m) matrix W and (m) vector of funcs, apply funcs to W.\n",
        "\n",
        "    Arguments:\n",
        "        W:  (n, m) matrix\n",
        "        funcs: list of activation functions (SymPy functions)\n",
        "        n_double:   Number of activation functions that take in 2 inputs\n",
        "\n",
        "    Returns:\n",
        "        SymPy matrix with 1 column that represents the output of applying the activation functions.\n",
        "    \"\"\"\n",
        "    W = sym.Matrix(W)\n",
        "    if n_double == 0:\n",
        "        for i in range(W.shape[0]):\n",
        "            for j in range(W.shape[1]):\n",
        "                W[i, j] = funcs[j](W[i, j])\n",
        "    else:\n",
        "        W_new = W.copy()\n",
        "        out_size = len(funcs)\n",
        "        for i in range(W.shape[0]):\n",
        "            in_j = 0\n",
        "            out_j = 0\n",
        "            while out_j < out_size - n_double:\n",
        "                W_new[i, out_j] = funcs[out_j](W[i, in_j])\n",
        "                in_j += 1\n",
        "                out_j += 1\n",
        "            while out_j < out_size:\n",
        "                W_new[i, out_j] = funcs[out_j](W[i, in_j], W[i, in_j+1])\n",
        "                in_j += 2\n",
        "                out_j += 1\n",
        "        for i in range(n_double):\n",
        "            W_new.col_del(-1)\n",
        "        W = W_new\n",
        "    return W\n",
        "\n",
        "\n",
        "def sym_pp(W_list, funcs, var_names, threshold=0.01, n_double=0):\n",
        "    \"\"\"Pretty print the hidden layers (not the last layer) of the symbolic regression network\n",
        "\n",
        "    Arguments:\n",
        "        W_list: list of weight matrices for the hidden layers\n",
        "        funcs:  list of lambda functions using sympy. has the same size as W_list[i][j, :]\n",
        "        var_names: list of strings for names of variables\n",
        "        threshold: threshold for filtering expression. set to 0 for no filtering.\n",
        "        n_double:   Number of activation functions that take in 2 inputs\n",
        "\n",
        "    Returns:\n",
        "        Simplified sympy expression.\n",
        "    \"\"\"\n",
        "    vars = []\n",
        "    for var in var_names:\n",
        "        if isinstance(var, str):\n",
        "            vars.append(sym.Symbol(var))\n",
        "        else:\n",
        "            vars.append(var)\n",
        "    expr = sym.Matrix(vars).T\n",
        "    W_list = np.asarray(W_list)\n",
        "    for W in W_list:\n",
        "        W = filter_mat(sym.Matrix(W), threshold=threshold)\n",
        "        expr = expr * W\n",
        "        expr = apply_activation(expr, funcs, n_double=n_double)\n",
        "    # expr = expr * W_list[-1]\n",
        "    return expr\n",
        "\n",
        "\n",
        "def last_pp(eq, W):\n",
        "    \"\"\"Pretty print the last layer.\"\"\"\n",
        "    return eq * filter_mat(sym.Matrix(W))\n",
        "\n",
        "\n",
        "def network(weights, funcs, var_names, threshold=0.01):\n",
        "    \"\"\"Pretty print the entire symbolic regression network.\n",
        "\n",
        "    Arguments:\n",
        "        weights: list of weight matrices for the entire network\n",
        "        funcs:  list of lambda functions using sympy. has the same size as W_list[i][j, :]\n",
        "        var_names: list of strings for names of variables\n",
        "        threshold: threshold for filtering expression. set to 0 for no filtering.\n",
        "\n",
        "    Returns:\n",
        "        Simplified sympy expression.\"\"\"\n",
        "    n_double = count_double(funcs)\n",
        "    funcs = [func.sp for func in funcs]\n",
        "\n",
        "    expr = sym_pp(weights[:-1], funcs, var_names, threshold=threshold, n_double=n_double)\n",
        "    expr = last_pp(expr, weights[-1])\n",
        "    expr = expr[0, 0]\n",
        "    return expr\n",
        "\n",
        "\n",
        "def filter_mat(mat, threshold=0.01):\n",
        "    \"\"\"Remove elements of a matrix below a threshold.\"\"\"\n",
        "    for i in range(mat.shape[0]):\n",
        "        for j in range(mat.shape[1]):\n",
        "            if abs(mat[i, j]) < threshold:\n",
        "                mat[i, j] = 0\n",
        "    return mat\n",
        "\n",
        "\n",
        "def filter_expr(expr, threshold=0.01):\n",
        "    \"\"\"Remove additive terms with coefficient below threshold\n",
        "    TODO: Make more robust. This does not work in all cases.\"\"\"\n",
        "    expr_new = sym.Integer(0)\n",
        "    for arg in expr.args:\n",
        "        if arg.is_constant() and abs(arg) > threshold:   # hack way to check if it's a number\n",
        "            expr_new = expr_new + arg\n",
        "        elif not arg.is_constant() and abs(arg.args[0]) > threshold:\n",
        "            expr_new = expr_new + arg\n",
        "    return expr_new\n",
        "\n",
        "\n",
        "def filter_expr2(expr, threshold=0.01):\n",
        "    \"\"\"Sets all constants under threshold to 0\n",
        "    TODO: Test\"\"\"\n",
        "    for a in sym.preorder_traversal(expr):\n",
        "        if isinstance(a, sym.Float) and a < threshold:\n",
        "            expr = expr.subs(a, 0)\n",
        "    return expr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4fFkP4gNnOr"
      },
      "source": [
        "symbolic network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djsTogPwNpXx"
      },
      "source": [
        "# Constants for L0 Regularization\n",
        "BETA = 2 / 3\n",
        "GAMMA = -0.1\n",
        "ZETA = 1.1\n",
        "EPSILON = 1e-6\n",
        "\n",
        "\n",
        "class SymbolicLayer:\n",
        "    \"\"\"Neural network layer for symbolic regression where activation functions correspond to primitive functions.\n",
        "    Can take multi-input activation functions (like multiplication)\"\"\"\n",
        "    def __init__(self, funcs=None, initial_weight=None, variable=False, init_stddev=0.1):\n",
        "        \"\"\"\n",
        "\n",
        "        funcs: List of activation functions, using utils.functions\n",
        "        initial_weight: (Optional) Initial value for weight matrix\n",
        "        variable: Boolean of whether initial_weight is a variable or not\n",
        "        init_stddev: (Optional) if initial_weight isn't passed in, this is standard deviation of initial weight\n",
        "        \"\"\"\n",
        "\n",
        "        if funcs is None:\n",
        "            funcs = default_func\n",
        "        self.initial_weight = initial_weight\n",
        "        self.W = None       # Weight matrix\n",
        "        self.built = False  # Boolean whether weights have been initialized\n",
        "        if self.initial_weight is not None:     # use the given initial weight\n",
        "            with tf.name_scope(\"symbolic_layer\"):\n",
        "                if not variable:\n",
        "                    self.W = tf.Variable(self.initial_weight)\n",
        "                else:\n",
        "                    self.W = self.initial_weight\n",
        "            self.built = True\n",
        "\n",
        "        self.output = None  # Tensorflow tensor for layer output\n",
        "        self.init_stddev = init_stddev\n",
        "        self.n_funcs = len(funcs)           # Number of activation functions (and number of layer outputs)\n",
        "        self.funcs = [func.tf for func in funcs]        # Convert functions to list of Tensorflow functions\n",
        "        self.n_double = count_double(funcs)   # Number of activation functions that take 2 inputs\n",
        "        self.n_single = self.n_funcs - self.n_double    # Number of activation functions that take 1 input\n",
        "\n",
        "        self.out_dim = self.n_funcs + self.n_double\n",
        "\n",
        "    def build(self, in_dim):\n",
        "        \"\"\"Initialize weight matrix\"\"\"\n",
        "        self.W = tf.compat.v1.Variable(tf.random_normal(shape=[in_dim, self.out_dim], stddev=self.init_stddev)) #different than v2\n",
        "        print(self.W)\n",
        "        self.built = True\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Multiply by weight matrix and apply activation units\"\"\"\n",
        "        with tf.name_scope(\"symbolic_layer\"):\n",
        "            if not self.built:\n",
        "                self.build(x.shape[1].value)    # First dimension is batch size\n",
        "            g = tf.matmul(x, self.W)  # shape = (?, self.size)\n",
        "            self.output = []\n",
        "\n",
        "            in_i = 0    # input index\n",
        "            out_i = 0   # output index\n",
        "            # Apply functions with only a single input\n",
        "            while out_i < self.n_single:\n",
        "                self.output.append(self.funcs[out_i](g[:, in_i]))\n",
        "                in_i += 1\n",
        "                out_i += 1\n",
        "            # Apply functions that take 2 inputs and produce 1 output\n",
        "            while out_i < self.n_funcs:\n",
        "                self.output.append(self.funcs[out_i](g[:, in_i], g[:, in_i+1]))\n",
        "                in_i += 2\n",
        "                out_i += 1\n",
        "            self.output = tf.stack(self.output, axis=1)\n",
        "            return self.output\n",
        "\n",
        "    def get_weight(self):\n",
        "        return self.W\n",
        "\n",
        "\n",
        "class SymbolicLayerBias(SymbolicLayer):\n",
        "    \"\"\"SymbolicLayer with a bias term\"\"\"\n",
        "    def __init__(self, funcs=None, initial_weight=None, variable=False, init_stddev=0.1):\n",
        "        super().__init__(funcs, initial_weight, variable, init_stddev)\n",
        "        self.b = None\n",
        "\n",
        "    def build(self, in_dim):\n",
        "        super().build(in_dim)\n",
        "        self.b = tf.compat.v1.Variable(tf.ones(shape=self.n_funcs) * 0.01)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Multiply by weight matrix and apply activation units\"\"\"\n",
        "        super().__call__(x)\n",
        "        self.output += self.b\n",
        "        return self.output\n",
        "\n",
        "\n",
        "class SymbolicNet:\n",
        "    \"\"\"Symbolic regression network with multiple layers. Produces one output.\"\"\"\n",
        "    def __init__(self, symbolic_depth, funcs=None, initial_weights=None, initial_bias=None,\n",
        "                 variable=False, init_stddev=0.1):\n",
        "        self.depth = symbolic_depth     # Number of hidden layers\n",
        "        self.funcs = funcs\n",
        "        self.shape = (None, 1)\n",
        "        if initial_weights is not None:\n",
        "            self.symbolic_layers = [SymbolicLayer(funcs=funcs, initial_weight=initial_weights[i], variable=variable)\n",
        "                                    for i in range(self.depth)]\n",
        "            if not variable:\n",
        "                self.output_weight = tf.compat.v1.Variable(initial_weights[-1])\n",
        "            else:\n",
        "                self.output_weight = initial_weights[-1]\n",
        "        else:\n",
        "            # Each layer initializes its own weights\n",
        "            if isinstance(init_stddev, list):\n",
        "                self.symbolic_layers = [SymbolicLayer(funcs=funcs, init_stddev=init_stddev[i]) for i in range(self.depth)]\n",
        "            else:\n",
        "                self.symbolic_layers = [SymbolicLayer(funcs=funcs, init_stddev=init_stddev) for _ in range(self.depth)]\n",
        "            # Initialize weights for last layer (without activation functions)\n",
        "            self.output_weight = tf.compat.v1.Variable(tf.random_uniform(shape=(self.symbolic_layers[-1].n_funcs, 1)))\n",
        "\n",
        "    def build(self, input_dim):\n",
        "        in_dim = input_dim\n",
        "        for i in range(self.depth):\n",
        "            self.symbolic_layers[i].build(in_dim)\n",
        "            in_dim = self.symbolic_layers[i].n_funcs\n",
        "\n",
        "    def __call__(self, input):\n",
        "        self.shape = (int(input.shape[1]), 1)     # Dimensionality of the input\n",
        "        h = input\n",
        "        # Building hidden layers\n",
        "        for i in range(self.depth):\n",
        "            h = self.symbolic_layers[i](h)\n",
        "        # Final output (no activation units) of network\n",
        "        h = tf.matmul(h, self.output_weight)\n",
        "        return h\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"Return list of weight matrices\"\"\"\n",
        "        # First part is iterating over hidden weights. Then append the output weight.\n",
        "        return [self.symbolic_layers[i].W for i in range(self.depth)] + [self.output_weight]\n",
        "\n",
        "\n",
        "class MaskedSymbolicNet(SymbolicNet):\n",
        "    \"\"\"Symbolic regression network where weights below a threshold are set to 0 and frozen. In other words, we apply\n",
        "    a mask to the symbolic network to fine-tune the non-zero weights.\"\"\"\n",
        "    def __init__(self, sess, sr_unit, threshold=0.01):\n",
        "        # weights = sr_unit.get_weights()\n",
        "        # masked_weights = []\n",
        "        # for w_i in weights:\n",
        "        #     # w_i = tf.where(tf.abs(w_i) < threshold, tf.zeros_like(w_i), w_i)\n",
        "        #     # w_i = tf.where(tf.abs(w_i) < threshold, tf.stop_gradient(w_i), w_i)\n",
        "        #     masked_weights.append(w_i)\n",
        "\n",
        "        weights = sr_unit.get_weights()\n",
        "        masked_weights = []\n",
        "        for w_i in weights:\n",
        "            mask = tf.constant(sess.run(tf.math.abs(w_i) > threshold), dtype=tf.float32)\n",
        "            masked_weights.append(tf.math.multiply(w_i, mask))\n",
        "\n",
        "        super().__init__(sr_unit.depth, funcs=sr_unit.funcs, initial_weights=masked_weights, variable=True)\n",
        "        self.sr_unit = sr_unit\n",
        "\n",
        "\n",
        "class SymbolicLayerL0(SymbolicLayer):\n",
        "    def __init__(self, funcs=None, initial_weight=None, variable=False, init_stddev=0.1,\n",
        "                 bias=False, droprate_init=0.5, lamba=1.):\n",
        "        super().__init__(funcs, initial_weight, variable, init_stddev) #super computes thefunction itself\n",
        "        self.droprate_init = droprate_init if droprate_init != 0 else 0.5\n",
        "        self.use_bias = bias\n",
        "        self.lamba = lamba\n",
        "        self.bias = None\n",
        "        self.qz_log_alpha = None\n",
        "        self.in_dim = None\n",
        "        self.eps = None\n",
        "\n",
        "    def build(self, in_dim):\n",
        "        with tf.name_scope(\"symbolic_layer\"):\n",
        "            self.in_dim = in_dim\n",
        "            if self.W is None:\n",
        "                self.W = tf.Variable(tf.random.normal(shape=[in_dim, self.out_dim], stddev=self.init_stddev))\n",
        "            if self.use_bias:\n",
        "                self.bias = tf.Variable(0.1*tf.ones((1, self.out_dim)))\n",
        "            self.qz_log_alpha = tf.Variable(tf.random.normal(shape=(in_dim, self.out_dim),\n",
        "                                                             mean=tf.math.log(1-self.droprate_init) - tf.math.log(self.droprate_init),\n",
        "                                                             stddev=1e-2))\n",
        "\n",
        "    def quantile_concrete(self, u):\n",
        "        \"\"\"Quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = tf.sigmoid((tf.math.log(u) - tf.math.log(1.0-u) + self.qz_log_alpha) / BETA)\n",
        "        return y * (ZETA - GAMMA) + GAMMA\n",
        "\n",
        "    def sample_u(self, shape, reuse_u=False):\n",
        "        \"\"\"Uniform random numbers for concrete distribution\"\"\"\n",
        "        # print(\"Hello\")\n",
        "        if self.eps is None or not reuse_u:\n",
        "            self.eps = tf.random.uniform(shape=shape, minval=EPSILON, maxval=1.0 - EPSILON)\n",
        "        return self.eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Use the hard concrete distribution as described in https://arxiv.org/abs/1712.01312\"\"\"\n",
        "        if sample:\n",
        "            eps = self.sample_u((batch_size, self.in_dim, self.out_dim))\n",
        "            z = self.quantile_concrete(eps)\n",
        "            return tf.clip_by_value(z, 0, 1)\n",
        "        else:   # Mean of the hard concrete distribution\n",
        "            pi = tf.sigmoid(self.qz_log_alpha)\n",
        "            return tf.clip_by_value(pi * (ZETA - GAMMA) + GAMMA, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "    def get_z_mean(self):\n",
        "        \"\"\"Mean of the hard concrete distribution\"\"\"\n",
        "        pi = tf.sigmoid(self.qz_log_alpha)\n",
        "        return tf.clip_by_value(pi * (ZETA - GAMMA) + GAMMA, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "    def sample_weights(self, reuse_u=False):\n",
        "        z = self.quantile_concrete(self.sample_u((self.in_dim, self.out_dim), reuse_u=reuse_u))\n",
        "        mask = tf.clip_by_value(z, clip_value_min=0.0, clip_value_max=1.0)\n",
        "        return mask * self.W\n",
        "\n",
        "    def get_weight(self):\n",
        "        \"\"\"Deterministic value of weight based on mean of z\"\"\"\n",
        "        return self.W * self.get_z_mean()\n",
        "\n",
        "    def loss(self):\n",
        "        \"\"\"Regularization loss term\"\"\"\n",
        "        return tf.reduce_sum(tf.sigmoid(self.qz_log_alpha - BETA * tf.math.log(-GAMMA / ZETA)))\n",
        "\n",
        "    def __call__(self, x, sample=True, reuse_u=False):\n",
        "        \"\"\"Multiply by weight matrix and apply activation units\"\"\"\n",
        "\n",
        "        with tf.name_scope(\"symbolic_layer\"):\n",
        "            if self.W is None or self.qz_log_alpha is None:\n",
        "                self.build(x.shape[1])\n",
        "\n",
        "            if sample:\n",
        "                h = tf.matmul(x, self.sample_weights(reuse_u=reuse_u))\n",
        "            else:\n",
        "                print(\"correct\")\n",
        "                w = self.get_weight()\n",
        "                h = tf.matmul(x, w)\n",
        "\n",
        "            if self.use_bias:\n",
        "                h = h + self.bias\n",
        "\n",
        "            # shape of h = (?, self.n_funcs)\n",
        "\n",
        "            self.output = []\n",
        "            # apply a different activation unit to each column of h\n",
        "            in_i = 0    # input index\n",
        "            out_i = 0   # output index\n",
        "            # Apply functions with only a single input\n",
        "            while out_i < self.n_single:\n",
        "                self.output.append(self.funcs[out_i](h[:, in_i]))\n",
        "                in_i += 1\n",
        "                out_i += 1\n",
        "            # Apply functions that take 2 inputs and produce 1 output\n",
        "            while out_i < self.n_funcs:\n",
        "                self.output.append(self.funcs[out_i](h[:, in_i], h[:, in_i+1]))\n",
        "                in_i += 2\n",
        "                out_i += 1\n",
        "            self.output = tf.stack(self.output, axis=1)\n",
        "            return self.output\n",
        "\n",
        "\n",
        "class SymbolicNetL0(SymbolicNet):\n",
        "    \"\"\"Symbolic regression network with multiple layers. Produces one output.\"\"\"\n",
        "    def __init__(self, symbolic_depth, funcs=None, initial_weights=None, initial_bias=None,\n",
        "                 variable=False, init_stddev=0.1):\n",
        "        super().__init__(symbolic_depth, funcs, initial_weights, initial_bias, variable, init_stddev)\n",
        "        if initial_weights is not None:\n",
        "            self.symbolic_layers = [SymbolicLayerL0(funcs=funcs, initial_weight=initial_weights[i], variable=variable)\n",
        "                                    for i in range(self.depth)]\n",
        "            if not variable:\n",
        "                self.output_weight = tf.Variable(initial_weights[-1])\n",
        "            else:\n",
        "                self.output_weight = initial_weights[-1]\n",
        "        else:\n",
        "            # Each layer initializes its own weights\n",
        "            if isinstance(init_stddev, list):\n",
        "                self.symbolic_layers = [SymbolicLayerL0(funcs=funcs, init_stddev=init_stddev[i])\n",
        "                                        for i in range(self.depth)]\n",
        "            else:\n",
        "                self.symbolic_layers = [SymbolicLayerL0(funcs=funcs, init_stddev=init_stddev)\n",
        "                                        for _ in range(self.depth)]\n",
        "            # Initialize weights for last layer (without activation functions)\n",
        "            self.output_weight = tf.Variable(tf.random_uniform(shape=(self.symbolic_layers[-1].n_funcs, 1)))\n",
        "\n",
        "    def __call__(self, input, sample=True, reuse_u=False):\n",
        "        tf.compat.v1.enable_eager_execution()\n",
        "        self.shape = (int(input.shape[1]), 1)     # Dimensionality of the input\n",
        "        # connect output from previous layer to input of next layer\n",
        "        h = input\n",
        "        saved_h = [h]\n",
        "        for i in range(self.depth):\n",
        "            if i==0 or i == self.depth-1:\n",
        "                h = self.symbolic_layers[i](h, sample=sample, reuse_u=reuse_u)\n",
        "            else:\n",
        "                h = self.symbolic_layers[i](h, sample=sample, reuse_u=reuse_u)\n",
        "                h = tf.concat([h, saved_h[-1]], 0)\n",
        "            \n",
        "            saved_h.append(h)\n",
        "\n",
        "        # Final output (no activation units) of network\n",
        "        h = tf.matmul(h, self.output_weight)\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "    def get_loss(self):\n",
        "        return tf.reduce_sum([self.symbolic_layers[i].loss() for i in range(self.depth)])\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.get_symbolic_weights() + [self.get_output_weight()]\n",
        "\n",
        "    def get_symbolic_weights(self):\n",
        "        return [self.symbolic_layers[i].get_weight() for i in range(self.depth)]\n",
        "\n",
        "    def get_output_weight(self):\n",
        "        return self.output_weight\n",
        "\n",
        "\n",
        "class SymbolicCell(tf.keras.layers.SimpleRNNCell):\n",
        "    \"\"\"cell for use with tf.keras.layers.RNN, allowing us to build a recurrent network with the EQL network.\n",
        "    This is used for the propagating decoder in the dynamics architecture.\n",
        "    Assume two state variables: position and velocity.\"\"\"\n",
        "    state_size = 2\n",
        "    output_size = 2\n",
        "\n",
        "    def __init__(self, sym1, sym2):\n",
        "        # units: dimensionality of output space\n",
        "        super().__init__(units=self.output_size)\n",
        "        self.sym1 = sym1\n",
        "        self.sym2 = sym2\n",
        "\n",
        "    def call(self, inputs, state, training=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            inputs (at time t): shape (batch, feature)\n",
        "            state: [x], shape(x)=(batch, feature)\n",
        "            training:   Ignore this\n",
        "        \"\"\"\n",
        "        full_input = state[0] + inputs[:, :2]\n",
        "        full_input = tf.concat([full_input, inputs[:, 2:4]], axis=1)\n",
        "        output = tf.concat([self.sym1(full_input), self.sym2(full_input)], axis=1)\n",
        "        next_state = output\n",
        "        return output, next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLmoffsOmyBj"
      },
      "source": [
        "benchmark utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC6SGgw_mzub"
      },
      "source": [
        "var_names = [\"x\", \"y\", \"z\"]\n",
        "\n",
        "def l12_smooth(input_tensor, a=0.05):\n",
        "    \"\"\"Smoothed L1/2 norm\"\"\"\n",
        "    if type(input_tensor) == list:\n",
        "        return sum([l12_smooth(tensor) for tensor in input_tensor])\n",
        "\n",
        "    smooth_abs = tf.where(tf.abs(input_tensor) < a,\n",
        "                          tf.pow(input_tensor, 4)/(-8*a**3) + tf.square(input_tensor)*3/4/a + 3*a/8,\n",
        "                          tf.abs(input_tensor))\n",
        "    return tf.reduce_sum(tf.sqrt(smooth_abs))\n",
        "\n",
        "class Benchmark:\n",
        "    \"\"\"Benchmark object just holds the results directory (results_dir) to save to and the hyper-parameters. So it is\n",
        "    assumed all the results in results_dir share the same hyper-parameters. This is useful for benchmarking multiple\n",
        "    functions with the same hyper-parameters.\"\"\"\n",
        "    def __init__(self, results_dir, n_layers=2, reg_weight=5e-3, learning_rate=1e-2,\n",
        "                 n_epochs1=10001, n_epochs2=10001):\n",
        "        \"\"\"Set hyper-parameters\"\"\"\n",
        "        self.activation_funcs = [\n",
        "            *[functions.Constant()] * 2,\n",
        "            *[functions.Identity()] * 4,\n",
        "            *[functions.Square()] * 4,\n",
        "            *[functions.Sin()] * 2,\n",
        "            *[functions.Exp()] * 2,\n",
        "            *[functions.Sigmoid()] * 2,\n",
        "            *[functions.Product()] * 2\n",
        "        ]\n",
        "\n",
        "        self.n_layers = n_layers              # Number of hidden layers\n",
        "        self.reg_weight = reg_weight     # Regularization weight\n",
        "        self.learning_rate = learning_rate\n",
        "        self.summary_step = 1000    # Number of iterations at which to print to screen\n",
        "        self.n_epochs1 = n_epochs1\n",
        "        self.n_epochs2 = n_epochs2\n",
        "\n",
        "        if not os.path.exists(results_dir):\n",
        "            os.makedirs(results_dir)\n",
        "        self.results_dir = results_dir\n",
        "\n",
        "        # Save hyperparameters to file\n",
        "        result = {\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"summary_step\": self.summary_step,\n",
        "            \"n_epochs1\": self.n_epochs1,\n",
        "            \"n_epochs2\": self.n_epochs2,\n",
        "            \"activation_funcs_name\": [func.name for func in self.activation_funcs],\n",
        "            \"n_layers\": self.n_layers,\n",
        "            \"reg_weight\": self.reg_weight,\n",
        "        }\n",
        "        with open(os.path.join(self.results_dir, 'params.pickle'), \"wb+\") as f:\n",
        "            pickle.dump(result, f)\n",
        "\n",
        "    def benchmark(self, func_dim, func_name, trials, x_comp, y_comp, split): #x_comp, y_comp are compilations of xset, yset\n",
        "        \"\"\"Benchmark the EQL network on data generated by the given function. Print the results ordered by test error.\n",
        "\n",
        "        Arguments:\n",
        "            func_dim: dimension of dataset\n",
        "            func_name: string that describes the function - this will be the directory name\n",
        "            trials: number of trials to train from scratch. Will save the results for each trial.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Starting benchmark for function:\\t%s\" % func_name)\n",
        "        print(\"==============================================\")\n",
        "\n",
        "        # Create a new sub-directory just for the specific function\n",
        "        func_dir = os.path.join(self.results_dir, func_name)\n",
        "        if not os.path.exists(func_dir):\n",
        "            os.makedirs(func_dir)\n",
        "\n",
        "        ##################### Train network! #######################\n",
        "        expr_list, error_test_list = self.train(x_comp=x_comp, y_comp=y_comp, split=split,\n",
        "                                                func_dim=func_dim, func_name=func_name,\n",
        "                                                trials=trials, func_dir=func_dir)\n",
        "        \n",
        "        #instead of being lists, let's just do them normally\n",
        "\n",
        "        # Sort the results by test error (increasing) and print them to file\n",
        "        # This allows us to easily count how many times it fit correctly.\n",
        "\n",
        "\n",
        "        error_expr_sorted = sorted(zip(error_test_list, expr_list))     # List of (error, expr)\n",
        "        error_test_sorted = [x for x, _ in error_expr_sorted]   # Separating out the errors\n",
        "        expr_list_sorted = [x for _, x in error_expr_sorted]    # Separating out the expr\n",
        "\n",
        "        fi = open(os.path.join(self.results_dir, 'eq_summary.txt'), 'a')\n",
        "        fi.write(\"\\n{}\\n\".format(func_name))\n",
        "        for i in range(trials):\n",
        "            fi.write(\"[%f]\\t\\t%s\\n\" % (error_test_sorted[i], str(expr_list_sorted[i])))\n",
        "        fi.close()\n",
        "\n",
        "        #knowing that the compiled x gives the real x... \n",
        "\n",
        "        final_expression = expr_list_sorted[0] #sympy\n",
        "        final_predict = generate_results(final_expression, func_dim, x_comp)\n",
        "\n",
        "        return final_predict\n",
        "              \n",
        "    def generate_results(self, expression, func_dim, x_comp):\n",
        "        # full_x = np.append(x_comp[0], x_comp[1], axis=0)\n",
        "\n",
        "        returnList = np.array([[]])\n",
        "        n = x_comp.get_shape().as_list()[0]\n",
        "\n",
        "        for i in range(n):\n",
        "          a = x_comp[i][0]\n",
        "          b = x_comp[i][1]\n",
        "          c = x_comp[i][2]\n",
        "\n",
        "          evaluate = np.array([[final_expression.sp.subs([(x,a),(y,b),(z,c)])]])\n",
        "          returnList = np.append(returnList, evaluate, axis=0)\n",
        "\n",
        "        return returnList\n",
        "\n",
        "    def train(self, x_comp, y_comp, split, func_dim, func_name='', trials=1, func_dir='results/test'): # trains itself, this is the true L0 one\n",
        "        \"\"\"Train the network to find a given function\"\"\"\n",
        "\n",
        "        n = x_comp.get_shape().as_list()[0]\n",
        "\n",
        "        if n == None:\n",
        "          split_index = None\n",
        "        else:\n",
        "          split_index = int(split * n)\n",
        "\n",
        "        x_dim = func_dim #len(signature(func).parameters)  # Number of input arguments to the function\n",
        "        # Generate training data and test data\n",
        "\n",
        "        x, x_test = x_comp[:split_index], x_comp[split_index:]\n",
        "        y, y_test = y_comp[:split_index], y_comp[split_index:]\n",
        "\n",
        "        # x, y = generate_data(func, N_TRAIN)\n",
        "        # x_val, y_val = generate_data(func, N_VAL)\n",
        "        # x_test, y_test = generate_data(func, N_TEST, range_min=DOMAIN_TEST[0], range_max=DOMAIN_TEST[1])\n",
        "\n",
        "        # Setting up the symbolic regression network\n",
        "        x_placeholder = tf.compat.v1.placeholder(shape=(None, x_dim), dtype=tf.float32)\n",
        "        width = len(self.activation_funcs)\n",
        "        n_double = count_double(self.activation_funcs)\n",
        "        sym = SymbolicNetL0(self.n_layers, funcs=self.activation_funcs,\n",
        "                            initial_weights=[\n",
        "                                                 tf.random.truncated_normal([x_dim, width + n_double], stddev=init_sd_first, dtype=tf.float32), #removed the dtypes\n",
        "                                                 tf.random.truncated_normal([width, width + n_double], stddev=init_sd_middle, dtype=tf.float32),\n",
        "                                                 tf.random.truncated_normal([width, width + n_double], stddev=init_sd_middle, dtype=tf.float32),\n",
        "                                                 tf.random.truncated_normal([width, 1], stddev=init_sd_last, dtype=tf.float32)\n",
        "                                             ], )\n",
        "        y_hat = sym(x_placeholder)\n",
        "\n",
        "        # Label and errors\n",
        "        error = tf.keras.losses.mean_squared_error(y_true=y, y_pred=y_hat)\n",
        "        error_test = tf.keras.losses.mean_squared_error(y_true=y_test, y_pred=y_hat)\n",
        "        # Regularization oscillates as a function of epoch.\n",
        "        reg_loss = sym.get_loss()\n",
        "        loss = error + self.reg_weight * reg_loss\n",
        "\n",
        "        # Training\n",
        "        learning_rate = tf.compat.v1.placeholder(tf.float32)\n",
        "        opt = tf.compat.v1.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
        "        train = opt.minimize(loss)\n",
        "\n",
        "        loss_list = []  # Total loss (MSE + regularization)\n",
        "        error_list = []     # MSE\n",
        "        reg_list = []       # Regularization\n",
        "        error_test_list = []    # Test error\n",
        "\n",
        "        error_test_final = []\n",
        "        eq_list = []\n",
        "\n",
        "        # Only take GPU memory as needed - allows multiple jobs on a single GPU\n",
        "        config = tf.compat.v1.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        with tf.compat.v1.Session(config=config) as sess:\n",
        "            for trial in range(trials):\n",
        "                print(\"Training on function \" + func_name + \" Trial \" + str(trial+1) + \" out of \" + str(trials))\n",
        "\n",
        "                loss_val = np.nan\n",
        "                # Restart training if loss goes to NaN (which happens when gradients blow up)\n",
        "                while np.all(np.isnan(loss_val)):\n",
        "                    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "                    # 1st stage of training with oscillating regularization weight\n",
        "                    for i in range(self.n_epochs1):\n",
        "                        feed_dict = {x_placeholder: x, learning_rate: self.learning_rate}\n",
        "                        _ = sess.run(train, feed_dict=feed_dict)\n",
        "                        if i % self.summary_step == 0:\n",
        "                            loss_val, error_val, reg_val, = sess.run((loss, error, reg_loss), feed_dict=feed_dict)\n",
        "                            error_test_val = sess.run(error_test, feed_dict={x_placeholder: x_test})\n",
        "\n",
        "                            loss_val_avg = sum(loss_val) / len(loss_val)\n",
        "                            error_test_val_avg = sum(error_test_val) / len(error_test_val)\n",
        "                            error_val_avg = sum(error_val) / len(error_val)\n",
        "\n",
        "                            print(error_val_avg)\n",
        "\n",
        "                            print(\"Epoch: %d\\tTotal training loss: %f\\tTest error: %f\" % (i, loss_val_avg, error_test_val_avg))\n",
        "                            loss_list.append(loss_val_avg)\n",
        "                            error_list.append(error_val_avg)\n",
        "                            reg_list.append(reg_val)\n",
        "                            error_test_list.append(error_test_val_avg)\n",
        "                            if np.any(np.isnan(loss_val)):  # If loss goes to NaN, restart training\n",
        "                                break\n",
        "\n",
        "                # Print the expressions\n",
        "                weights = sess.run(sym.get_weights())\n",
        "                expr = network(weights, self.activation_funcs, var_names[:x_dim])\n",
        "                print(expr)\n",
        "\n",
        "                # Save results\n",
        "                trial_file = os.path.join(func_dir, 'trial%d.pickle' % trial)\n",
        "\n",
        "                results = {\n",
        "                    \"weights\": weights,\n",
        "                    \"loss_list\": loss_list,\n",
        "                    \"error_list\": error_list,\n",
        "                    \"reg_list\": reg_list,\n",
        "                    \"error_test\": error_test_list,\n",
        "                    \"expr\": expr\n",
        "                }\n",
        "\n",
        "                with open(trial_file, \"wb+\") as f:\n",
        "                    pickle.dump(results, f)\n",
        "\n",
        "                error_test_final.append(error_test_list[-1])\n",
        "                eq_list.append(expr)\n",
        "\n",
        "        return eq_list, error_test_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHcofmDUO8d5"
      },
      "source": [
        "BENCHMARK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jNvUxCgNKN0"
      },
      "source": [
        "# Network Below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VlzJFzoNSbT"
      },
      "source": [
        "import pickle\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# N_TRAIN = 256       # Size of training dataset\n",
        "# N_VAL = 100         # Size of validation dataset\n",
        "# DOMAIN = (-1, 1)    # Domain of dataset\n",
        "# # DOMAIN = np.array([[0, -1, -1], [1, 1, 1]])   # Use this format if each input variable has a different domain\n",
        "# N_TEST = 100        # Size of test dataset\n",
        "# DOMAIN_TEST = (-2, 2)   # Domain of test dataset - should be larger than training domain to test extrapolation\n",
        "# NOISE_SD = 0        # Standard deviation of noise for training dataset\n",
        "var_names = [\"x\", \"y\", \"z\"]\n",
        "\n",
        "# Standard deviation of random distribution for weight initializations.\n",
        "init_sd_first = 0.5\n",
        "init_sd_last = 0.5\n",
        "init_sd_middle = 0.5\n",
        "\n",
        "\n",
        "# generate_data = benchmark.generate_data\n",
        "\n",
        "\n",
        "class BenchmarkReal(Benchmark): # call BenchmarkReal.benchmark() to actually start things\n",
        "    \"\"\"Benchmark object just holds the results directory (results_dir) to save to and the hyper-parameters. So it is\n",
        "    assumed all the results in results_dir share the same hyper-parameters. This is useful for benchmarking multiple\n",
        "    functions with the same hyper-parameters.\"\"\"\n",
        "    def __init__(self, results_dir, n_layers=2, reg_weight=1e-2, learning_rate=1e-2,\n",
        "                 n_epochs1=20001, n_epochs2=10001):\n",
        "        \"\"\"Set hyper-parameters\"\"\"\n",
        "        self.activation_funcs = [\n",
        "            *[Constant()] * 2,\n",
        "            *[Identity()] * 4,\n",
        "            *[Square()] * 4,\n",
        "            *[Sin()] * 2,\n",
        "            *[Exp()] * 2, #something wrong with the exponent\n",
        "            *[Sigmoid()] * 2,\n",
        "            *[Product()] * 2\n",
        "        ]\n",
        "\n",
        "        self.n_layers = n_layers              # Number of hidden layers\n",
        "        self.reg_weight = reg_weight     # Regularization weight\n",
        "        self.learning_rate = learning_rate\n",
        "        self.summary_step = 1000    # Number of iterations at which to print to screen\n",
        "        self.n_epochs1 = n_epochs1\n",
        "        self.n_epochs2 = n_epochs2\n",
        "\n",
        "        if not os.path.exists(results_dir):\n",
        "            os.makedirs(results_dir)\n",
        "        self.results_dir = results_dir\n",
        "\n",
        "        # Save hyperparameters to file\n",
        "        result = {\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"summary_step\": self.summary_step,\n",
        "            \"n_epochs1\": self.n_epochs1,\n",
        "            \"n_epochs2\": self.n_epochs2,\n",
        "            \"activation_funcs_name\": [func.name for func in self.activation_funcs],\n",
        "            \"n_layers\": self.n_layers,\n",
        "            \"reg_weight\": self.reg_weight,\n",
        "        }\n",
        "        with open(os.path.join(self.results_dir, 'params.pickle'), \"wb+\") as f:\n",
        "            pickle.dump(result, f)\n",
        "\n",
        "\n",
        "eql_setup = BenchmarkReal(results_dir=params[\"results_dir\"],\n",
        "                          n_layers=params[\"n_layers\"],\n",
        "                          reg_weight=params[\"reg_weight\"],\n",
        "                          learning_rate=params[\"learning_rate_1\"],\n",
        "                          n_epochs1=params[\"n_epochs1\"],\n",
        "                          n_epochs2=params[\"n_epochs2\"])\n",
        "\n",
        "\n",
        "# going into the network is x, y, z autoencoder predict, coming out is equations\n",
        "# predicting dx dy dz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lZo1d4sIDP1"
      },
      "source": [
        "#constructing network, replacing sindy with CNN\n",
        "#fix all the np array tensor switcharound\n",
        "\n",
        "#full network\n",
        "\n",
        "# def y_composite(y, func_dim, splice):\n",
        "#   n = splice*len(y)\n",
        "#   returnList = []\n",
        "#   for i in range(func_dim):\n",
        "#     stored = []\n",
        "#     arranged = y[:,[i]]\n",
        "#     spliced_f = arranged[:n]\n",
        "#     spliced_b = arranged[n:]\n",
        "\n",
        "#     stored = [spliced_f, spliced_b]\n",
        "#     returnList.append(stored)\n",
        "\n",
        "#   return returnList\n",
        "    \n",
        "def full_network(params):\n",
        "    \"\"\"\n",
        "    Define the full network architecture.\n",
        "\n",
        "    Arguments:\n",
        "        params - Dictionary object containing the parameters that specify the training.\n",
        "        See README file for a description of the parameters.\n",
        "\n",
        "    Returns:\n",
        "        network - Dictionary containing the tensorflow objects that make up the network.\n",
        "    \"\"\"\n",
        "    eql_test_frac = params[\"split\"]\n",
        "\n",
        "    input_dim = params['input_dim']\n",
        "    latent_dim = params['latent_dim']\n",
        "    activation = params['activation']\n",
        "    poly_order = params['poly_order']\n",
        "    if 'include_sine' in params.keys():\n",
        "        include_sine = params['include_sine']\n",
        "    else:\n",
        "        include_sine = False\n",
        "    model_order = params['model_order']\n",
        "\n",
        "    network = {}\n",
        "\n",
        "    x = tf.compat.v1.placeholder(tf.float32, shape=[None, input_dim], name='x')\n",
        "    dx = tf.compat.v1.placeholder(tf.float32, shape=[None, input_dim], name='dx')\n",
        "\n",
        "    z, x_decode, encoder_weights, encoder_biases, decoder_weights, decoder_biases = nonlinear_autoencoder(x, input_dim, latent_dim, params['widths'], activation=activation)\n",
        "\n",
        "    \n",
        "    #splicing and presetting\n",
        "\n",
        "    # print(z)\n",
        "    # z_shape = tf.compat.v1.placeholder(tf.float21, shape=[2])\n",
        "    # sliced_z = z[:z_shape[0]]\n",
        "    # length_splice_z = int(np.floor(z.shape[0] * eql_test_frac))\n",
        "    # print(length_splice_z)\n",
        "\n",
        "    # training_z = z[0:length_splice_z]\n",
        "    # testing_z = z[length_splice_z:]\n",
        "\n",
        "    dz = z_derivative(x, dx, encoder_weights, encoder_biases, activation=activation) # this just finds the z derivative assuming the same node transformation \n",
        "\n",
        "\n",
        "    y_comp_1, y_comp_2, y_comp_3 = dz[:,0], dz[:,1], dz[:,2]\n",
        "\n",
        "    print(z)\n",
        "\n",
        "    dz_predict_x = eql_setup.benchmark(func_dim=3, func_name=\"Lorenz_x\", trials=1, x_comp=z, y_comp=y_comp_1, split=eql_test_frac)\n",
        "    dz_predict_y = eql_setup.benchmark(func_dim=3, func_name=\"Lorenz_y\", trials=1, x_comp=z, y_comp=y_comp_2, split=eql_test_frac)\n",
        "    dz_predict_z = eql_setup.benchmark(func_dim=3, func_name=\"Lorenz_z\", trials=1, x_comp=z, y_comp=y_comp_3, split=eql_test_frac)\n",
        "\n",
        "    eql_predict = np.append(dz_predict_x, dz_predict_y, dz_predict_z, axis=1)\n",
        "\n",
        "    dx_decode = z_derivative(z, eql_predict, decoder_weights, decoder_biases, activation=activation) #finds the decoded dx from the other direction\n",
        "\n",
        "    network['x'] = x\n",
        "    network['dx'] = dx\n",
        "    network['z'] = z\n",
        "    network['dz'] = dz\n",
        "    network['x_decode'] = x_decode\n",
        "    network['dx_decode'] = dx_decode\n",
        "    network['encoder_weights'] = encoder_weights\n",
        "    network['encoder_biases'] = encoder_biases\n",
        "    network['decoder_weights'] = decoder_weights\n",
        "    network['decoder_biases'] = decoder_biases\n",
        "    network[\"dz_predict\"] = eql_predict\n",
        "\n",
        "    return network\n",
        "\n",
        "\n",
        "#loss functions\n",
        "def define_loss(network, params):\n",
        "    \"\"\"\n",
        "    Create the loss functions.\n",
        "\n",
        "    Arguments:\n",
        "        network - Dictionary object containing the elements of the network architecture.\n",
        "        This will be the output of the full_network() function.\n",
        "    \"\"\"\n",
        "    x = network['x']\n",
        "    x_decode = network['x_decode']\n",
        "    dz = network['dz']\n",
        "    dz_predict = network['dz_predict']\n",
        "    dx = network['dx']\n",
        "    dx_decode = network['dx_decode']\n",
        "\n",
        "    losses = {}\n",
        "    losses['decoder'] = tf.reduce_mean((x - x_decode)**2)\n",
        "    losses['eql_z'] = tf.reduce_mean((dz - dz_predict)**2)\n",
        "    losses['eql_x'] = tf.reduce_mean((dx - dx_decode)**2)\n",
        "    loss = params['loss_weight_decoder'] * losses['decoder'] \\\n",
        "           + params['loss_weight_eql_z'] * losses['eql_z'] \\\n",
        "           + params['loss_weight_eql_x'] * losses['eql_x']\n",
        "\n",
        "    loss_refinement = params['loss_weight_decoder'] * losses['decoder'] \\\n",
        "                      + params['loss_weight_eql_z'] * losses['eql_z'] \\\n",
        "                      + params['loss_weight_eql_x'] * losses['eql_x']\n",
        "\n",
        "    return loss, losses, loss_refinement\n",
        "    #loss_refinement is w/o sindy reg\n",
        "\n",
        "\n",
        "\n",
        "#actual autoencoder parts\n",
        "def nonlinear_autoencoder(x, input_dim, latent_dim, widths, activation='elu'):\n",
        "  \"returns a list of encoder, decoder weights and biases\"\n",
        "  if activation == 'relu':\n",
        "    activation_function = tf.nn.relu\n",
        "  elif activation == 'elu':\n",
        "      activation_function = tf.nn.elu\n",
        "  elif activation == 'sigmoid':\n",
        "      activation_function = tf.sigmoid\n",
        "  else:\n",
        "    raise ValueError('invalid activation function')\n",
        "    \n",
        "  z,encoder_weights,encoder_biases = build_network_layers(x, input_dim, latent_dim, widths, activation_function, 'encoder')\n",
        "  x_decode,decoder_weights,decoder_biases = build_network_layers(z, latent_dim, input_dim, widths[::-1], activation_function, 'decoder')\n",
        "\n",
        "\n",
        "  return z, x_decode, encoder_weights, encoder_biases, decoder_weights, decoder_biases\n",
        "\n",
        "\n",
        "def build_network_layers(input, input_dim, output_dim, widths, activation, name):\n",
        "  \"128 to 3 layers, with hidden layer of 376 inside\"\n",
        "  \"W begins as a 128 x 128 shape, then gets multiplied to a 128, then etc etc\"\n",
        "  \"widths = represent how many units are in each hidden layer\"\n",
        "  \"builds an encoder, decoder\"\n",
        "\n",
        "  weights = []\n",
        "  biases = []\n",
        "  last_width=input_dim\n",
        "  for i,n_units in enumerate(widths):\n",
        "    W0 = tf.compat.v1.get_variable(name+'_W'+str(i), shape=[last_width,n_units],\n",
        "            initializer=tf.random_normal_initializer(seed=1))\n",
        "    \n",
        "    W = tf.nn.dropout(W0, rate=0.3)\n",
        "    b = tf.compat.v1.get_variable(name+'_b'+str(i), shape=[n_units],\n",
        "            initializer=tf.constant_initializer(0.0))\n",
        "    \n",
        "    input = tf.matmul(input, W) + b\n",
        "    if activation is not None:\n",
        "      input = activation(input)\n",
        "\n",
        "    last_width = n_units\n",
        "    weights.append(W)\n",
        "    biases.append(b)\n",
        "\n",
        "  W0 = tf.compat.v1.get_variable(name+'_W'+str(len(widths)), shape=[last_width,output_dim],\n",
        "      initializer=tf.random_normal_initializer(seed=1))\n",
        "  b = tf.compat.v1.get_variable(name+'_b'+str(len(widths)), shape=[output_dim],\n",
        "      initializer=tf.random_normal_initializer(seed=1))\n",
        "  \n",
        "  W = tf.nn.dropout(W0, rate=0.3)\n",
        "\n",
        "  input = tf.matmul(input,W) + b\n",
        "  weights.append(W)\n",
        "  biases.append(b)\n",
        "  return input, weights, biases\n",
        "\n",
        "\n",
        "#z derivative\n",
        "def z_derivative(input, dx, weights, biases, activation='elu'):\n",
        "    dz = dx\n",
        "    if activation == 'elu':\n",
        "        for i in range(len(weights)-1):\n",
        "            input = tf.matmul(input, weights[i]) + biases[i]\n",
        "            dz = tf.multiply(tf.minimum(tf.exp(input),1.0),\n",
        "                                  tf.matmul(dz, weights[i]))\n",
        "            input = tf.nn.elu(input)\n",
        "        dz = tf.matmul(dz, weights[-1])\n",
        "    elif activation == 'relu':\n",
        "        for i in range(len(weights)-1):\n",
        "            input = tf.matmul(input, weights[i]) + biases[i]\n",
        "            dz = tf.multiply(tf.to_float(input>0), tf.matmul(dz, weights[i]))\n",
        "            input = tf.nn.relu(input)\n",
        "        dz = tf.matmul(dz, weights[-1])\n",
        "    elif activation == 'sigmoid':\n",
        "        for i in range(len(weights)-1):\n",
        "            input = tf.matmul(input, weights[i]) + biases[i]\n",
        "            input = tf.sigmoid(input)\n",
        "            dz = tf.multiply(tf.multiply(input, 1-input), tf.matmul(dz, weights[i]))\n",
        "        dz = tf.matmul(dz, weights[-1])\n",
        "    else:\n",
        "        for i in range(len(weights)-1):\n",
        "            dz = tf.matmul(dz, weights[i])\n",
        "        dz = tf.matmul(dz, weights[-1])\n",
        "    return dz\n",
        "\n",
        "\n",
        "#eql things\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Veao4ZfjTfB"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "def train_network(training_data, val_data, params):\n",
        "    # SET UP NETWORK\n",
        "    autoencoder_network = full_network(params)\n",
        "    loss, losses, loss_refinement = define_loss(autoencoder_network, params)\n",
        "    learning_rate = tf.compat.v1.placeholder(tf.float32, name='learning_rate')\n",
        "    train_op = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "    train_op_refinement = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_refinement)\n",
        "    saver = tf.compat.v1.train.Saver(var_list=tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES))\n",
        "\n",
        "    validation_dict = create_feed_dictionary(val_data, params, idxs=None)\n",
        "\n",
        "    x_norm = np.mean(val_data['x']**2)\n",
        "    nn_norm_x = np.mean(val_data['dx']**2)\n",
        "\n",
        "    validation_losses = []\n",
        "\n",
        "    print('TRAINING')\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        for i in range(params['max_epochs']):\n",
        "            for j in range(params['epoch_size']//params['batch_size']):\n",
        "                batch_idxs = np.arange(j*params['batch_size'], (j+1)*params['batch_size'])\n",
        "                train_dict = create_feed_dictionary(training_data, params, idxs=batch_idxs)\n",
        "                sess.run(train_op, feed_dict=train_dict)\n",
        "            \n",
        "            if params['print_progress'] and (i % params['print_frequency'] == 0):\n",
        "                validation_losses.append(print_progress(sess, i, loss, losses, train_dict, validation_dict, x_norm, nn_norm_x))\n",
        "\n",
        "        print('REFINEMENT')\n",
        "        for i_refinement in range(params['refinement_epochs']):\n",
        "            for j in range(params['epoch_size']//params['batch_size']):\n",
        "                batch_idxs = np.arange(j*params['batch_size'], (j+1)*params['batch_size'])\n",
        "                train_dict = create_feed_dictionary(training_data, params, idxs=batch_idxs)\n",
        "                sess.run(train_op_refinement, feed_dict=train_dict)\n",
        "            \n",
        "            if params['print_progress'] and (i_refinement % params['print_frequency'] == 0):\n",
        "                validation_losses.append(print_progress(sess, i_refinement, loss_refinement, losses, train_dict, validation_dict, x_norm, nn_norm_x))\n",
        "\n",
        "        saver.save(sess, params['data_path'] + params['save_name'])\n",
        "        pickle.dump(params, open(params['data_path'] + params['save_name'] + '_params.pkl', 'wb'))\n",
        "        final_losses = sess.run((losses['decoder'], losses['eql_x'], losses['eql_z']),\n",
        "                                feed_dict=validation_dict)\n",
        "        if params['model_order'] == 1:\n",
        "            eql_norm_z = np.mean(sess.run(autoencoder_network['dz'], feed_dict=validation_dict)**2)\n",
        "        else:\n",
        "            eql_norm_z = np.mean(sess.run(autoencoder_network['ddz'], feed_dict=validation_dict)**2)\n",
        "\n",
        "        results_dict = {}\n",
        "        results_dict['num_epochs'] = i\n",
        "        results_dict['x_norm'] = x_norm\n",
        "        results_dict['eql_norm_x'] = eql_norm_x\n",
        "        results_dict['eql_norm_z'] = eql_norm_z\n",
        "        results_dict['loss_decoder'] = final_losses[0]\n",
        "        results_dict['loss_decoder_eql'] = final_losses[1]\n",
        "        results_dict['loss_eql'] = final_losses[2]\n",
        "        results_dict['validation_losses'] = np.array(validation_losses)\n",
        "        results_dict[\"autoencoder\"] = autoencoder_network\n",
        "\n",
        "        return results_dict\n",
        "\n",
        "\n",
        "def print_progress(sess, i, loss, losses, train_dict, validation_dict, x_norm, nn_norm):\n",
        "    \"\"\"\n",
        "    Print loss function values to keep track of the training progress.\n",
        "\n",
        "    Arguments:\n",
        "        sess - the tensorflow session\n",
        "        i - the training iteration\n",
        "        loss - tensorflow object representing the total loss function used in training\n",
        "        losses - tuple of the individual losses that make up the total loss\n",
        "        train_dict - feed dictionary of training data\n",
        "        validation_dict - feed dictionary of validation data\n",
        "        x_norm - float, the mean square value of the input\n",
        "        nn_norm - float, the mean square value of the time derivatives of the input.\n",
        "        Can be first or second order time derivatives depending on the model order.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of losses calculated on the validation set.\n",
        "    \"\"\"\n",
        "    training_loss_vals = sess.run((loss,) + tuple(losses.values()), feed_dict=train_dict)\n",
        "    validation_loss_vals = sess.run((loss,) + tuple(losses.values()), feed_dict=validation_dict)\n",
        "    print(\"Epoch %d\" % i)\n",
        "    print(\"   training loss {0}, {1}\".format(training_loss_vals[0],\n",
        "                                             training_loss_vals[1:]))\n",
        "    print(\"   validation loss {0}, {1}\".format(validation_loss_vals[0],\n",
        "                                               validation_loss_vals[1:]))\n",
        "    decoder_losses = sess.run((losses['decoder'], losses['eql_x']), feed_dict=validation_dict)\n",
        "    loss_ratios = (decoder_losses[0]/x_norm, decoder_losses[1]/eql_norm)\n",
        "    print(\"decoder loss ratio: %f, decoder EQL loss  ratio: %f\" % loss_ratios)\n",
        "    return validation_loss_vals\n",
        "\n",
        "\n",
        "def create_feed_dictionary(data, params, idxs=None):\n",
        "    \"\"\"\n",
        "    Create the feed dictionary for passing into tensorflow.\n",
        "\n",
        "    Arguments:\n",
        "        data - Dictionary object containing the data to be passed in. Must contain input data x,\n",
        "        along the first (and possibly second) order time derivatives dx (ddx).\n",
        "        params - Dictionary object containing model and training parameters. The relevant\n",
        "        parameters are model_order (which determines whether the SINDy model predicts first or\n",
        "        second order time derivatives), sequential_thresholding (which indicates whether or not\n",
        "        coefficient thresholding is performed), coefficient_mask (optional if sequential\n",
        "        thresholding is performed; 0/1 mask that selects the relevant coefficients in the SINDy\n",
        "        model), and learning rate (float that determines the learning rate).\n",
        "        idxs - Optional array of indices that selects which examples from the dataset are passed\n",
        "        in to tensorflow. If None, all examples are used.\n",
        "\n",
        "    Returns:\n",
        "        feed_dict - Dictionary object containing the relevant data to pass to tensorflow.\n",
        "    \"\"\"\n",
        "    if idxs is None:\n",
        "        idxs = np.arange(data['x'].shape[0])\n",
        "    feed_dict = {}\n",
        "    feed_dict['x:0'] = data['x'][idxs]\n",
        "    feed_dict['dx:0'] = data['dx'][idxs]\n",
        "    if params['model_order'] == 2:\n",
        "        feed_dict['ddx:0'] = data['ddx'][idxs]\n",
        "    feed_dict['learning_rate:0'] = params['learning_rate']\n",
        "    return feed_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CwLCkL4OpZ0_",
        "outputId": "f03bf79b-a177-4294-b425-c1792f6a1dd1"
      },
      "source": [
        "num_experiments = 1\n",
        "df = pd.DataFrame()\n",
        "for i in range(num_experiments):\n",
        "    print('EXPERIMENT %d' % i)\n",
        "\n",
        "    params['save_name'] = 'lorenz_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "\n",
        "    results_dict = train_network(training_data, validation_data, params)\n",
        "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
        "\n",
        "#df.to_pickle('experiment_results_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M\") + '.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXPERIMENT 0\n",
            "Tensor(\"add_2:0\", shape=(None, 3), dtype=float32)\n",
            "Tensor(\"add_2:0\", shape=(None, 3), dtype=float32)\n",
            "Starting benchmark for function:\tLorenz_x\n",
            "==============================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-9f09be997cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresults_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-4f418d1df073>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(training_data, val_data, params)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# SET UP NETWORK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mautoencoder_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_refinement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-00c1b6de2ec3>\u001b[0m in \u001b[0;36mfull_network\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mdz_predict_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meql_setup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Lorenz_x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_comp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_comp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_comp_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meql_test_frac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mdz_predict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meql_setup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Lorenz_y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_comp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_comp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_comp_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meql_test_frac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mdz_predict_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meql_setup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Lorenz_z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_comp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_comp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_comp_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meql_test_frac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-106-d6388ca570bc>\u001b[0m in \u001b[0;36mbenchmark\u001b[0;34m(self, func_dim, func_name, trials, x_comp, y_comp, split)\u001b[0m\n\u001b[1;32m     72\u001b[0m         expr_list, error_test_list = self.train(x_comp=x_comp, y_comp=y_comp, split=split,\n\u001b[1;32m     73\u001b[0m                                                 \u001b[0mfunc_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                                                 trials=trials, func_dir=func_dir)\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m#instead of being lists, let's just do them normally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-106-d6388ca570bc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_comp, y_comp, split, func_dim, func_name, trials, func_dir)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                                  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_sd_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                                              ], )\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Label and errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-43b64597b9ee>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input, sample, reuse_u)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse_u\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Dimensionality of the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# connect output from previous layer to input of next layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5865\u001b[0m         \u001b[0mdevice_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5866\u001b[0m         \u001b[0mexecution_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5867\u001b[0;31m         server_def=None)\n\u001b[0m\u001b[1;32m   5868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution_internal\u001b[0;34m(config, device_policy, execution_mode, server_def)\u001b[0m\n\u001b[1;32m   5924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph_mode_has_been_used\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5925\u001b[0m       raise ValueError(\n\u001b[0;32m-> 5926\u001b[0;31m           \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5927\u001b[0m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_execution_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEAGER_MODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5928\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VHwNBIcDUv3"
      },
      "source": [
        "#visualizing data-- let's run the first entry of the validation set through the\n",
        "#now-trained autoencoder, then do something with the results... how will we do\n",
        "#this?? i have no idea lmfao\n",
        "\n",
        "#lowdim = validation_data[\"s\"]\n",
        "\n",
        "rawdata = results_dict[\"autoencoder\"]\n",
        "#print(rawdata)\n",
        "\n",
        "#nn_dz = rawdata[\"dz_predict\"]\n",
        "#decoder_dz = rawdata[\"dz\"]\n",
        "#decoder_z = rawdata[\"z\"]\n",
        "\n",
        "print(params[\"save_name\"])\n",
        "#print(params[\"data_path\"])\n",
        "\n",
        "\n",
        "#a = tf.constant([[1, 2], [3, 4]])                 \n",
        "#b = tf.add(nn_dz, 1)\n",
        "#out = tf.multiply(nn_dz, b)\n",
        "#print(nn_dz)\n",
        "#print(b)\n",
        "#print(out.eval(session=tf.compat.v1.Session()))   \n",
        "\n",
        "#print(tf.reduce_mean(nn_dz-decoder_dz))\n",
        "\n",
        "\n",
        "#with tf.compat.v1.Session() as sess:\n",
        "#  sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  \n",
        "\n",
        "\n",
        "#fig_val = plt.figure()\n",
        "#ax_val = fig_val.add_subplot(111, projection='3d')\n",
        "#ax_val.plot(lowdim[0,:,0], lowdim[0,:,1], lowdim[0,:,2], \"b--\")\n",
        "\n",
        "#origin = lowdim[0,0]\n",
        "#transformed = [origin]\n",
        "\n",
        "#for i in range(len(lowdim)):\n",
        "#  a = transformed[i] + 0.01*deriv[i]\n",
        "#  ax_val.plot(a[0], a[1], a[2], \"r--\")\n",
        "#  transformed.append(a)\n",
        "\n",
        "#plt.xticks([])\n",
        "#plt.axis('off')\n",
        "#ax_val.view_init(azim=120)\n",
        "\n",
        "\n",
        "\n",
        "#fig_valp = plt.figure()\n",
        "#ax_valp = fig_valp.add_subplot(111, projection='3d')\n",
        "#ax_valp.plot(lowdim_predict[:,0], lowdim_predict[:,1], lowdim_predict[:,2], \"r--\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y680y1yl_hdX"
      },
      "source": [
        "data_path = os.getcwd() + '/'\n",
        "#save_name = 'model1'\n",
        "params = pickle.load(open(\"/content/lorenz_2021_01_29_01_05_20_133196_params.pkl\", 'rb'))\n",
        "params['save_name'] = data_path + params[\"save_name\"]\n",
        "\n",
        "print(params[\"save_name\"])\n",
        "\n",
        "autoencoder_network = rawdata\n",
        "\n",
        "learning_rate = tf.compat.v1.placeholder(tf.float32, name='learning_rate')\n",
        "saver = tf.compat.v1.train.Saver(var_list=tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES))\n",
        "\n",
        "tensorflow_run_tuple = ()\n",
        "for key in autoencoder_network.keys():\n",
        "    tensorflow_run_tuple += (autoencoder_network[key],)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB-iUUDL6LO4"
      },
      "source": [
        "#yay\n",
        "\n",
        "t = np.arange(0,20,.01)\n",
        "z0 = np.array([[-8,7,27]])\n",
        "\n",
        "test_data = generate_lorenz(z0, t, params['input_dim'], linear=False, normalization=np.array([1/40,1/40,1/40]))\n",
        "\n",
        "test_data['s'] = test_data['s'].reshape((-1,params['latent_dim'])) \n",
        "test_data['ds'] = test_data['ds'].reshape((-1,params['latent_dim']))\n",
        "test_data['x'] = test_data['x'].reshape((-1,params['input_dim'])) #gotta reshape the x,dx as well to map to dict\n",
        "test_data['dx'] = test_data['dx'].reshape((-1,params['input_dim']))\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    saver.restore(sess, params[\"save_name\"])\n",
        "    test_dictionary = create_feed_dictionary(test_data, params)\n",
        "    tf_results = sess.run(tensorflow_run_tuple, feed_dict=test_dictionary)\n",
        "\n",
        "test_set_results = {}\n",
        "for i,key in enumerate(autoencoder_network.keys()):\n",
        "    test_set_results[key] = tf_results[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5uroEXDDkbL"
      },
      "source": [
        "from scipy.optimize import fsolve\n",
        "import math\n",
        "\n",
        "\n",
        "final_z = test_set_results[\"z\"]\n",
        "final_dz = test_set_results[\"dz\"]\n",
        "\n",
        "fig_val = plt.figure(figsize=(10,10),facecolor='w')\n",
        "ax_val = fig_val.add_subplot(111, projection='3d')\n",
        "\n",
        "ax_val.plot(final_dz[:,0], final_dz[:,1], final_dz[:,2], \"r--\")\n",
        "\n",
        "predictions = test_set_results[\"dz_predict\"]\n",
        "\n",
        "a = np.array([final_z[0]])\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  [l, m, n] = predictions[i]\n",
        "\n",
        "  def lorenz_1(vars,du=l, dv=m, dw=n):\n",
        "    u, v, w = vars\n",
        "    eq1 = -10*(u - v) - du\n",
        "    eq2 = 8/3 *u - v - u*w - dv\n",
        "    eq3 = -28*w + u*v - dw\n",
        "    return eq1, eq2, eq3\n",
        "\n",
        "  u,v,w = np.array(fsolve(lorenz_1, tuple(final_z[i])))\n",
        "\n",
        "  q = np.array([u,v,w])\n",
        "  a = np.append(a, q).reshape(i+2, 3)\n",
        "\n",
        "\n",
        "# ax_val.plot(a[:,0], a[:,1], a[:,2], \"r--\")\n",
        "#ax_val.plot(predictions[:,0], predictions[:,1], predictions[:,2], \"r--\")\n",
        "\n",
        "real_diff = test_data[\"ds\"]\n",
        "\n",
        "print(predictions)\n",
        "# ax_val.plot(real_diff[:,0], real_diff[:,1], real_diff[:,2], \"b--\")\n",
        "plt.xticks([])\n",
        "plt.axis('off')\n",
        "ax_val.view_init(elev=60, azim=90)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}